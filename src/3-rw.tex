\chapter{Related Work}
\label{cha:rw}

\section{Dataflow Stream Processing}

Applications over streaming data can be implemented using
high-performance, fault tolerant stream processing systems, such as
Flink \cite{Flink2015,Flink2017,Flink}, Trill~\cite{chandramouli2014trill},
IBM Streams~\cite{HAG2013SPL},
Spark Streaming~\cite{Spark2013,SparkStreaming}, Storm~\cite{Storm},
Samza~\cite{Samza2017}, Heron~\cite{kulkarni2015twitter-heron,Heron},
MillWheel~\cite{MillWheel},
Beam~\cite{Beam},
and Naiad~\cite{Naiad2013,Timely}.
The core programming model for these systems is typically based on dataflow with autoparallelization, as discussed in~\Cref{cha:background},
and this is the primary point of comparison for this thesis.
However, there are other programming models used for streaming, including
high-level query languages (often based on SQL),
extensions to the dataflow model,
and extensions to stream parallelism and distribution.

% Cut
% Stream processing research spans three layers of abstraction in the software
% stack: at the lowest level, stream processing systems handle distributed runtime and operational concerns; at the middle layer, the core programming model represents the semantic model the system uses to represent and parallelize streams (as discussed in~\Cref{cha:background}), and at the highest layer, query languages such as those based on SQL usually compile down to or are implemented as primitives in the core programming model.

\subsection{High-level Query Languages}

High-level query languages for streaming include Streaming SQL~\cite{jain2008towards,begoli2019one}, CQL~\cite{arasu2003cql,ABW2006CQL}, SamzaSQL~\cite{pathirage2016samzasql}, Structured Streaming~\cite{armbrust2018structured},
and StreamQRE~\cite{StreamQRE}.
High-level query languages have existed since the early history
of streaming research from the databases community~\cite{stonebraker20058}.
They offer the promise of convenience and clean semantics, but they can be expressively limited for some use cases (e.g., the value-barrier example discussed in the introduction), as we discuss in~\citeMain{ppopp22}.
They typically offer type-safety with respect to the relational schema of each stream and determinism with respect to processing order --
though there are cases where determinism is not always guaranteed, including in some possible implementations of CQL's tuple-based windows where ties must be broken arbitrarily.\footnote{Thank you to Phillip Hilliard for this observation.}
Traditional query languages view streams in the \emph{sequence-of-relations} model popularized by CQL, which is limited in expressiveness compared to general partial orders.
Tuple-based windows are an example of where this limit is evident: there is no way to have a stream which is a true sequence of events at the type level, as there is always the possibility that multiple events could arrive concurrently at the same timestamp.
% Joins can also be highly expensive, and in the worst case, unbounded as there is no guarantee that a join can be parallelized and optimized effectively.

In modern systems, query operators (typically including maps, windows, filters, joins, and aggregates)
are often implemented not as a separate stream management platform but as
operators on top of the core dataflow programming model, which can also support custom stateful processing and where data need not conform to the relational schema viewpoint.
The upshot of this design is that the parallelism present at the core dataflow programming model level is highly relevant even for query languages as it dictates the manner in which streams are distributed and optimized.

Types are useful across the software stack, whether at the dataflow model level or at the query language level.
This is evident in \Cref{cha:distribution} where we pass types from the distributed programming model to the distributed runtime and use them to implement automatic safe distribution.
The presentation of \Cref{cha:foundation}'s type system in~\citeMain{pods21} was inspired by database schema types at the query language level.

\subsection{Dataflow Programming}

Datflow programming goes back to Kahn Process Networks~\cite{gilles1974semantics} (KPN), which is a deterministic dataflow model based on the restriction that channels are FIFO queues with blocking reads and non-blocking writes. Today's systems wish to offer on-demand processing and avoid arbitrary buffering, so they typically do not implement the KPN approach for streams, but there is one restriction of KPN's that has been hugely influential, synchronous dataflow~\cite{lee1987synchronous}, which further restricts the FIFO queues so that a fixed number of items are read and written on each cycle of an operator. StreamIt~\cite{thies2002streamit} shows that this leads to massive opportunities in optimization and scheduling, and it in a sense eliminates the problem with blocking reads because all input and output is known statically. The work on synchronous languages from the 90s including LUSTRE~\cite{halbwachs1991synchronous} and ESTEREL~\cite{berry1992esterel} (see~\cite{BCEHlGdS2003SL} for an overview) similarly benefits from this synchronous assumption.

The problem with traditional synchronous dataflow  languages is that they cannot implement operators which produce an non-static number of output items in response to an input -- including the very most basic such operator, filter. Filter of an input stream discards items that do not match a predicate. As a result, the synchronous model is generally considered to restrictive today for general streaming~\cite{schneider2013safe}.

A more modern take on dataflow programming is MapReduce online~\cite{condie2010mapreduce}; this exemplifies the viewpoint that streaming dataflow graphs are like MapReduce operators chained together.
Other works on streaming specifically focusing on the programming model include SPADE~\cite{gedik2008spade} and the SP Calculus~\cite{soule2010universal}.
% TODO: ellaborate...

\subsection{Extensions to the Dataflow Model}

The dataflow model is limited in its ability to express iterative or recursive queries and other queries with periodic synchronization between nodes, leading to various extensions for more general distributed programming.
Naiad~\cite{Naiad2013,Timely} proposes \emph{timely dataflow} in order
to support iterative computation.
However, our evaluation in \Cref{cha:distribution} shows that timely dataflow falls short of automatically scaling without high-level design sacrifices (exposing implementation details to the user).

S-Store and TSpoon~\cite{meehan2015s,affetti2020tspoon} extend stream processing systems with online transaction processing (OLTP), which can implement concurrency control.
Nova~\cite{zhao2021timestamped} extends stream processing systems with a shared state abstraction.

\subsection{Distribution Strategies}

On the other side of the programming model,
researchers have proposed distribution, parallelization, and optimization strategies for stream programs.
These are relevant as they affect the sort of semantic guarantees that the system can offer with respect to parallelism.

A fundamental problem in distribution of streaming operators, and closely related to the distribution problem considered in \Cref{cha:distribution},
is \emph{operator placement} where the system determines what node to run a stream operator on.
The work~\cite{cardellini2016optimal} uses constraint solving to optimize operator placement relative to network bandwidth and other constraints.
Other than~\cite{cardellini2016optimal}, there are several lines of work in job scheduling, operator placement, and optimization for DSPS that try to be network-aware in some fashion. Early and influential works include~\cite{ahmad2004network} and~\cite{pietzuch2006network}.
The first~\cite{ahmad2004network} is probably the first to formalize the operator placement problem for DSPS, and to explore (1) how network-awareness can lead to more efficient query evaluation, and (2) how there is a trade-off between latency and bandwidth use in this space.
The second~\cite{pietzuch2006network} presents a simple but effective heuristic algorithm which treats the geo-distributed physical nodes as a system of points in a combined latency-bandwidth space, and uses spring relaxation find a good configuration.
The work~\cite{gu2015general}, similarly to~\cite{cardellini2016optimal}, formulates the problem in MILP.
There are a number of other related papers on job scheduling~\cite{aniello2013adaptive,xu2014tstorm,eidenbenz2016task,wolf2008soda,fu2019edgewise}, operator placement~\cite{bonfils2004adaptive,tziritas2016improving,rizou2010solving,lakshmanan2008placement},
and resource elasticity~\cite{cardellini2018decentralized,hochreiner2016elastic,cardellini2018optimal,dias2018dsp-survey}.

Researchers have also modified the programming model to enable distribution and parallelism.
SpanEdge~\cite{sajjad2016spanedge} is a primitive modification to the dataflow programming model with tasks that should be run globally versus locally.
A system very related to SpanEdge is Geelytics~\cite{cheng2015geelytics}.
It modifies the dataflow programming model with \emph{scoped tasks}, which have a geographic granularity such as by site, by city, by district, or by section, and are similar to SpanEdge's local and global tasks.
Other than these, the paper~\cite{renart2017datadrivenstreamedge} proposes a programming framework for stream processing in a geo-distributed (at the edge) fashion. The programming framework, however, is not based on dataflow, and is more focused on the communication mechanisms between nodes. There is also a large body of work on programming for wireless sensor networks; see the survey~\cite{mottola2011programming-wsn}. In general, the concerns in that domain have been more low-level, related to connections between sensors, mobility of sensors, communication from one sensor to another via short hops, and so on.

Another approach to obtaining distribution of streaming programs is even more aggressive: it calls for degrading and approximating streams to reduce what needs to be sent over the network.
AWStream~\cite{zhang2018awstream} uses programming knobs to control the amount of degredation that occurs for video streams (e.g. frame rate reduction, resolution reduction, or some combination) to try to achieve a Pareto-optimal solution between accuracy and bandwidth use.
The first stream processing system to incorporate the degradation of data streams as found in AWStream was JetStream~\cite{rabkin2014jetstream}.
JetStream seeks to limit bandwidth use not just through degradation, but also ``aggregation'', which refers to a data model where data is saved and aggregated by geo-distributed nodes, and only sent when explicitly requested.
An early idea in DSPS that predates degradation is \emph{load shedding}~\cite{tatbul2003load,tatbul2007staying}.
This refers to selectively dropping data in response to load that is beyond capacity, in order to maintain availability and good latency while hopefully not losing too much accuracy.

\section{Related Paradigms}

\subsection{Actor Models}

As data processing applications are becoming more complex, evolving from
data analytics to general event-driven applications, some stream
processing and database systems are moving from dataflow programming to more
general actor models
\cite{CarboneFKK20,Bernstein19,BernsteinDKM17,Das2018,xu2021move}.
For example, Flink has recently released Stateful Functions,
an actor-based programming model running on top of Flink
\cite{AkhterFK19,StatefulFunctions}.
Actor models can encode arbitrary synchronization patterns,
but the patterns still need to be implemented manually as
message-passing protocols.

\subsection{Concurrent and Distributed Programming Models}

Fork-join based
concurrent programming~\cite{frigo1998implementation,lea2000java},
bringing some of the expressiveness in those models to the streaming setting,
where parallelism is much less flexible but essential for performance,
but also extending them, since in our setting the system (and not the user) decides when to fork and join by choosing a synchronization plan.
A particularly relevant example is
Concurrent Revisions~\cite{burckhardt2010concurrent},
which is a programming model that guarantees determinism in the presence of concurrent updates by allowing programmers to specify isolation types that are processed in parallel and then merged at join points.
The difference of our work is that it targets a more restricted domain providing automation,
not requiring programmers to manually specify the execution synchronization points.
Another related domain is monotonic lattice-based programming models,
including
Conflict-Free Replicated Data Types~\cite{shapiro2011conflict},
Bloom$^L$~\cite{conway12},
and LVars~\cite{lvars13,lvars14},
which are designed for coordination-free distributed programming.
These models guarantee strong eventual consistency,
i.e., eventually all replicas will have the same state,
but, in contrast to our model, CRDTs and Bloom$^L$
do not allow synchronization between different workers.
LVars, which focuses on determinism for concurrent updates on shared variables,
extends lattice-based models with a freeze operation that enforces a synchronization point,
inducing partial order executions that are similar to the ones in our model.
Some similarities with our work can be found in the domain of consistency for replicated data stores.
Some examples include RedBlue consistency~\cite{li2012making},
MixT~\cite{milano2018mixt},
Quelea~\cite{sivaramakrishnan2015declarative},
CISE~\cite{gotsman16},
Carol~\cite{lewchenko2019sequential},
all of which support a mix of consistency guarantees on different operations,
inducing a partial order of data store operations.
Fearless concurrency~\cite{milano2022flexible} is a recent proposal for concurrent programming which guarantees safe execution.
% TODO this paragraph is a mess

\subsection{Runtime Verification and Monitoring}

Our work on DiffStream (\Cref{cha:testing}) contributes to the large body of work on runtime verification~\cite{leucker2009brief,havelund2004efficient}, a
lightweight verification paradigm which aims to identify bugs in the
output of a program as it is executed, using minimal computational
resources.  Most work in runtime verification focuses on detecting
violations of a property written in a logical specification language
(e.g., the temporal logic LTL and its extensions), whereas we consider
differential testing of a program against a reference implementation,
and we model program execution traces as partially rather than totally
ordered.

Runtime monitoring (see the survey \cite{LS2009RV}) is a lightweight verification technique for testing whether a finite execution trace of a system satisfies a given specification. The specification is translated into a \emph{monitor}, which executes along with the monitored system: it consumes system events in a streaming manner and outputs the satisfaction or falsification of the specification. A widely used formalism for describing specifications for monitoring is \emph{Linear Temporal Logic} (LTL) \cite{havelund2004efficient}. Metric Temporal Logic (MTL) has been used for monitoring real-time temporal properties \cite{TR2005MTL}. Signal Temporal Logic (STL), which extends MTL with value comparisons, has been used for monitoring real-valued signals \cite{DDGJJS2017}.
Computing statistical aggregates of LTL-defined properties, as in \cite{finkbeiner2002collecting}, is a limited form of \emph{quantitative} monitoring.
The Eagle specification language \cite{barringer2004rule} can also express some quantitative monitoring properties, since it supports data-bindings.
% However, prior formalisms for monitoring either completely lack quantitative features or they do not allow a rich set of quantitative operations as we do here.

The synchronous languages~\cite{BCEHlGdS2003SL} mentioned earlier are also used for processing data streams.
Examples of synchronous languages designed for runtime monitoring include LOLA \cite{d2005lola,bozzelli2016foundations}.
% Todo: and RTLola

% \subsection{Reactive Programming}
% TODO WRITE THIS
% FRP

% \subsection{Programmable Networks}
% TODO WRITE THIS

\section{Correctness Support for Data-Parallel Programs}

\subsection{Testing}

Many previous works focus on batch processing programs written in the MapReduce~\cite{dean2008mapreduce} framework \cite{csallner2011new,xu2013semantic,marynowski2012testing,chen2016commutativity} (see also the recent survey \cite{moran2019testing}). Going beyond batch processing, \cite{xu2013testing} study testing semantic properties of operators in general dataflow or stream-processing programs.
One problem with many of these works \cite{csallner2011new,xu2013semantic,xu2013testing,chen2016commutativity} is that real-world MapReduce programs (and, by extension, aggregators in stream processing programs) can be non-commutative: the empirical study at Microsoft~\cite{xiao2014nondeterminism} reports that about 58\% of 507 user-written reduce jobs are non-commutative, and that most of these are most likely not buggy. The previous work on testing would erroneously flag such programs as containing bugs due to nondeterminism, which would generate a large number of false positives.
We adopt a black-box differential testing approach with the goal of avoiding this problem. Concretely, we have shown in \Cref{cha:testing} how to avoid a false positive for most cases where the application requirements imply that the nondeterminism is acceptable.

Differential testing~\cite{mckeeman1998differential,groce2007randomized} is a well-established, lightweight, and scalable way to detect bugs in complex programs (for instance, in C compilers~\cite{yang2011finding}), by simply comparing two programs that are supposed to be equivalent.
% TODO umm need this?

\subsection{Static Verification}

In addition to testing---a dynamic method of checking
correctness---there has also been research on the static verification
of data-parallel programs. Recent work focuses on the verification of
parallel aggregators that are used in MapReduce programs; either by
enabling automated verification and synthesis of \emph{partial
aggregators} given an aggregation function~\cite{liu2014automating},
or by parallelizing user defined aggregators using symbolic
execution~\cite{raychev2015parallelizing}. Both of these works help
developers by statically providing guarantees about the correctness of
parallel aggregator functions. DiffStream complements these approaches
by checking the correctness of general stateful streaming programs,
which are not always decomposable into aggregators, and whose parallel
and sequential implementations might have significant structural
differences (like the Topic Count case study in
\Cref{cha:testing}), implying that the parallel implementation cannot be simply
derived from the sequential implementation. Finally, another
difference is that DiffStream (and dynamic approaches in general) can be
used on programs that interact with external services (e.g. the Redis
database in \Cref{cha:testing}) without having to model
them---as is the case with static approaches.

For general stream processing, \cite{schneider2013safe} has proposed an approach to ensure correct parallelization based on categorizing operators for properties such as statefulness and selectivity.
This is very closely related to our work on guaranteeing type-safe, deterministic distribution.
% TODO fix OMG

\subsection{Empirical Studies}

There are a number of empirical studies which aim to classify bugs in real-world stream- and batch-processing programs. Of these, most~\cite{schroeder2009large, kavulya2010analysis, li2013characteristic, zhou2015empirical} have primarily focused on sources of job failures (e.g., system crashes) or performance issues (e.g., memory use patterns and computational bottlenecks), which are orthogonal to semantic bugs which can be found by testing. The Microsoft study~\cite{xiao2014nondeterminism} is the only study we are aware of that classifies semantic bugs in user-written programs.
In addition to these studies of data-processing programs, there have been some empirical studies which interview users about their testing and debugging needs. In~\cite{fisher2012interactions}, users of Spark are interviewed about tools that would be useful to them, but the study focuses on \emph{human-computer interaction} needs such as data visualization and debugging tools. The more recent study~\cite{vianna2019exploratory} aims to determine how current specialists in data stream processing applications currently implement testing. Most specialist employ unit and integration testing, together with some techniques and tools for more sophisticated testing (e.g. simulating system failures). Our work is motivated by the need to go beyond these techniques which are standard in software engineering, to increase confidence in \emph{semantic correctness} of user-written programs, especially in the presence of parallelism and out-of-order data.

\subsection{Other Approaches}

Complementary to directly establishing the correctness of user-written programs, there is work on indirectly facilitating correctness through visualization, through debugging, and finally by ensuring correctness at the system level.
Visualization includes generating example inputs for dataflow programs showcasing typical semantic behavior~\cite{olston2009generating}. Debugging includes, e.g., setting up breakpoints, stepping through computations, and determining crash culprits~\cite{gulzar2016bigdebug,olston2011inspector}.
Towards testing functional correctness of a stream processing system implementation, a framework has been proposed for Microsoft StreamInsight~\cite{raizman2010extensible}.

\section{Partially Ordered Trace Theory}

We build on foundational work in concurrency theory dating back to Mazurkiewicz \cite{mazurkiewicz1986trace}, where partially ordered sets of events are called \emph{Mazurkiewicz traces}. Mazurkiewicz traces have been studied from the
viewpoint of algebra, combinatorics, formal languages and automata, and
logic \cite{DiekertR1995}. In practical applications to verification and
testing of concurrent systems, they appear in relation to
\emph{partial order reduction}~\cite{God96,Peled94}, a technique for
pruning the search space of possible execution sequences.
The idea of a dependence relation to specify output ordering originally comes from Mazurkiewicz traces; however, the core algorithmic problem in our work corresponds to checking \emph{equivalence} of two Mazurkiewicz traces, and to our knowledge this particular testing problem has not been studied in any of the mentioned contexts.
An additional difference is that in the theory of Mazurkiewicz traces, one usually assumes a finite, symmetric, and reflexive dependence relation. In contrast, we only require it to be symmetric. This is in order to support user-provided dependence relations over a possibly infinite data domain, which is necessary to model common patterns in the streaming setting: one example is the key-based dependence relation (where the number of keys may be unbounded and different keys are independent). Patterns such as this one cannot be captured by a finite alphabet, and this limits the direct application of classical work on concurrency theory over a finite dependence relation, e.g.~\cite{DiekertR1995}.

\subsection{Checking Properties of Traces}

Much classical research has focused on deciding properties of traces such as serializability, linearizability, sequential consistency, and data race detection.
Broadly speaking, these properties are search problems: the algorithm monitors an execution of events, and it must decide if there exists some possible equivalent execution that witnesses the desired property. For example, race detection involves deciding, given a sequence of events, if there is a valid reordering of the events, subject to the constraints imposed by synchronization events, in which two specific events (representing a potential race condition) get reordered. The search aspect means that race detection is NP-hard~\cite{netzer1990complexity,netzer1992race}.
Similarly, checking sequential consistency of a given trace is NP-complete~\cite{gibbons1992complexity},
as is checking linearizability in general~\cite{gibbons1997testing}.
As a result, practical tools for testing correctness of traces (e.g., \cite{savage1997eraser,park2011efficient,sen2008race,wing1993testing,burckhardt2010line,lowe2017testing}) must explore the trade-off between soundness, completeness, and tractability.
In contrast, the problem we consider of checking two traces for equivalence up to re-ordering is in PTIME (for the offline variant), and admits an optimal online monitoring algorithm.

\section{Streaming Models of Computation}

% TODO fix, update heading here
% The literature contains various proposals of automata-based models that are some kind of quantitative extension of classical finite-state automata.

\emph{Weighted automata}, which were introduced in \cite{S1961WA} (see also the more recent monograph \cite{DKV2009HWA}), extend nondeterministic finite-state automata by annotating transitions with \emph{weights} (which are elements of a semiring) and can be used for the computation of simple quantitative properties. A weighted automaton maps an input string $w$ to the minimum over costs of all accepting paths of the automaton over $w$.
Extensions such as \emph{nested weighted automata} \cite{CHO2015NWA} enjoy increased expressiveness, but fall short of capturing an arbitrary set of data types and operations as CRAs and DTs do. We recently studied arbitrary hierarchical nesting of weighted automata in \cite{AMS2017SA}, which does allow arbitrary types and operations. We showed that under certain typing restrictions there is a streaming evaluation algorithm. In contrast, here we introduce a model that admits streaming evaluation \emph{sans} typing restrictions; which is ``flat'', i.e. not recursively defined; such that the transition structure makes modular composition feasible; and for which we have clean succinctness results.

Another approach to augment classical automata with quantitative features has been with the addition of \emph{registers} that can store values from a potentially infinite set. These models are typically varied in two aspects: by the choice of data types and operations that are allowed for register manipulation, and by the ability to perform tests on the registers for control flow.

The literature on data words, data/register automata and their associated logics \cite{KF1994FMA, NSV2004FSM, DL2009LFQ, BS2010NRDL, BDMSS2011LDW} studies models that operate on words over an infinite alphabet, which is typically of the form $\Sigma \times \mathbb{N}$, where $\Sigma$ is a finite set of tags and $\mathbb{N}$ is the set of the natural numbers. They allow comparing data values for equality, and these equality tests can affect the control flow.
In DTs, tests on the data are not allowed to affect the underlying control flow, that is, whether each state variable is undefined, defined, or conflicted.

The work on cost register automata (CRAs) \cite{AdADRY2013CRA, AR2013ARF} and streaming transducers \cite{AC2010SST, AC2011STA, AdA2012STT} is about models where the control and data registers are kept separate by allowing write access to the registers but no testing. As discussed in \S\ref{dt:subsec:dts-and-cras}, DTs are exponentially more succinct than CRAs. The exponential gap arises for the useful construction of performing several subcomputations in parallel and combining their results. DTs recognize the class of \emph{streamable regular transductions}, which
is equivalently defined by CRAs and attribute grammars \cite{arXiv2018}.

The recent work \cite{BDK2018} gives a characterization of the first-order definable and MSO-definable string-to-string transformations using algebras of functions that operate on objects such as lists, lists of lists, pairs of lists, lists of pairs of lists, and so on. Monitors with finite-state control and unbounded integer registers are studied in \cite{FHS2018} and a hierarchy of expressiveness is established on the basis of the number of available registers. These papers focus on issues related to expressiveness, whereas we focus here on modularity and succinctness.
