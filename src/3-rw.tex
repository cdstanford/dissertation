\chapter{Related Work}
\label{cha:rw}

% \section{Dataflow Programming}
%
% \section{Query Languages and Streaming Databases}
%
% \section{Distributed Trace Theory}

%%%%%%%%%% FLUMINA RW %%%%%%%%%%

\section{Dataflow stream processing systems.}

Applications over streaming data can be implemented using
high-performance, fault tolerant stream processing systems, such as
Flink \cite{carbone2015flink}, Trill~\cite{chandramouli2014trill},
IBM Streams~\cite{HAG2013SPL},
Spark Streaming~\cite{DStreams2013}, Storm~\cite{Storm},
Samza~\cite{Samza2017}, Heron~\cite{kulkarni2015twitter-heron}, and
MillWheel~\cite{MillWheel}.
The need for synchronization in these systems has resulted in a number of extensions to their APIs, but they fall short of a general solution.
Naiad~\cite{murray2013naiad} proposes \emph{timely dataflow} in order
to support iterative computation, which enables some synchronization but falls short of automatically scaling without high-level design sacrifices, as shown in our evaluation.
S-Store and TSpoon~\cite{meehan2015s,affetti2020tspoon} extend stream processing systems with online transaction processing (OLTP),
which includes some forms of synchronization, e.g. locking-based concurrency control.
Concurrent with our work, Nova~\cite{zhao2021timestamped} also identifies
the need for synchronization in stream processing systems,
and proposes to address it through a shared state abstraction.

% See~\cite{venkataraman2017drizzle} for an example of a recent system that tries to attack the problem of balancing fast recovery time and low latency.

\section{Actor-based databases.}
As data processing applications are becoming more complex, evolving from
data analytics to general event-driven applications, some stream
processing and database systems are moving from dataflow programming to more
general actor models
\cite{DBLP:conf/sigmod/CarboneFKK20,DBLP:conf/cidr/Bernstein19,DBLP:conf/cidr/BernsteinDKM17,DBLP:conf/sigmod/0001S18,xu2021move}.
For example, Flink has recently released Stateful Functions,
an actor-based programming model running on top of Flink
\cite{DBLP:journals/pvldb/AkhterFK19,StatefulFunctions}.
Actor models can encode arbitrary synchronization patterns,
but the patterns still need to be implemented manually as
message-passing protocols.
DGS and synchronization plans can be built on top
of the actor abstraction, and in fact our own implementation
relies on actors as provided by Erlang~\cite{armstrong1993erlang}.

\section{Programming with synchronization.}

In the broader context of distributed and parallel programming,
synchronization is a significant source of overhead for developers,
and a good deal of existing work can be viewed as addressing this problem.
Our model draws inspiration from fork-join based
concurrent programming~\cite{frigo1998implementation,lea2000java},
  bringing some of the expressiveness in those models to the streaming setting,
  where parallelism is much less flexible but essential for performance,
  but also extending them, since in our setting the system (and not the user) decides when to fork and join by choosing a synchronization plan.
%
A particularly relevant example is
Concurrent Revisions~\cite{burckhardt2010concurrent},
which is a programming model that guarantees determinism in the presence of concurrent updates by allowing programmers to specify isolation types that are processed in parallel and then merged at join points.
The difference of our work is that it targets a more restricted domain providing automation,
not requiring programmers to manually specify the execution synchronization points.
Another related domain is monotonic lattice-based programming models,
including
Conflict-Free Replicated Data Types~\cite{shapiro2011conflict},
Bloom$^L$~\cite{conway12},
and LVars~\cite{lvars13,lvars14},
which are designed for coordination-free distributed programming.
These models guarantee strong eventual consistency,
i.e., eventually all replicas will have the same state,
but, in contrast to our model, CRDTs and Bloom$^L$
do not allow synchronization between different workers.
LVars, which focuses on determinism for concurrent updates on shared variables,
extends lattice-based models with a freeze operation that enforces a synchronization point,
inducing partial order executions that are similar to the ones in our model.
Some similarities with our work can be found in the domain of consistency for replicated data stores.
Some examples include RedBlue consistency~\cite{li2012making},
MixT~\cite{milano2018mixt},
Quelea~\cite{sivaramakrishnan2015declarative},
CISE~\cite{gotsman16},
Carol~\cite{lewchenko2019sequential},
all of which support a mix of consistency guarantees on different operations,
inducing a partial order of data store operations.

\section{Correctness in stream processing.}

Finally, in prior work,
researchers have pointed out that
parallelization in stream processing systems is not
semantics-preserving, and have proposed methods to restrict
parallelization so that it preserves the
semantics~\cite{schneider2015safeparallelism, mamouras2019data}.
In particular, dependence relations have been previously used in this context
as to specify and ensure correct parallelism~\cite{mamouras2019data,2020:DifferentialTesting:OOPSLA}
and as a type system for synchronization~\cite{alur2021synchronization}.
However, these works do not propose a general programming model
or generation of a parallel implementation.

%%%%%%%%%% POPL RW %%%%%%%%%%

\section{Quantitative Automata}

The literature contains various proposals of automata-based models that are some kind of quantitative extension of classical finite-state automata.

\emph{Weighted automata}, which were introduced in \cite{S1961WA} (see also the more recent monograph \cite{DKV2009HWA}), extend nondeterministic finite-state automata by annotating transitions with \emph{weights} (which are elements of a semiring) and can be used for the computation of simple quantitative properties. A weighted automaton maps an input string $w$ to the minimum over costs of all accepting paths of the automaton over $w$.
Extensions such as \emph{nested weighted automata} \cite{CHO2015NWA} enjoy increased expressiveness, but fall short of capturing an arbitrary set of data types and operations as CRAs and DTs do. We recently studied arbitrary hierarchical nesting of weighted automata in \cite{AMS2017SA}, which does allow arbitrary types and operations. We showed that under certain typing restrictions there is a streaming evaluation algorithm. In contrast, here we introduce a model that admits streaming evaluation \emph{sans} typing restrictions; which is ``flat'', i.e. not recursively defined; such that the transition structure makes modular composition feasible; and for which we have clean succinctness results.

Another approach to augment classical automata with quantitative features has been with the addition of \emph{registers} that can store values from a potentially infinite set. These models are typically varied in two aspects: by the choice of data types and operations that are allowed for register manipulation, and by the ability to perform tests on the registers for control flow.

The literature on data words, data/register automata and their associated logics \cite{KF1994FMA, NSV2004FSM, DL2009LFQ, BS2010NRDL, BDMSS2011LDW} studies models that operate on words over an infinite alphabet, which is typically of the form $\Sigma \times \mathbb{N}$, where $\Sigma$ is a finite set of tags and $\mathbb{N}$ is the set of the natural numbers. They allow comparing data values for equality, and these equality tests can affect the control flow.
In DTs, tests on the data are not allowed to affect the underlying control flow, that is, whether each state variable is undefined, defined, or conflicted (see Theorem~\ref{thm:regular-language}).

The work on cost register automata (CRAs) \cite{AdADRY2013CRA, AR2013ARF} and streaming transducers \cite{AC2010SST, AC2011STA, AdA2012STT} is about models where the control and data registers are kept separate by allowing write access to the registers but no testing. As discussed in \S\ref{subsec:dts-and-cras}, DTs are exponentially more succinct than CRAs. The exponential gap arises for the useful construction of performing several subcomputations in parallel and combining their results. DTs recognize the class of \emph{streamable regular transductions}, which
is equivalently defined by CRAs and attribute grammars \cite{arXiv2018}.

The recent work \cite{BDK2018} gives a characterization of the first-order definable and MSO-definable string-to-string transformations using algebras of functions that operate on objects such as lists, lists of lists, pairs of lists, lists of pairs of lists, and so on. Monitors with finite-state control and unbounded integer registers are studied in \cite{FHS2018} and a hierarchy of expressiveness is established on the basis of the number of available registers. These papers focus on issues related to expressiveness, whereas we focus here on modularity and succinctness.

\section{Query Languages for Runtime Monitoring}

Runtime monitoring (see the survey \cite{LS2009RV}) is a lightweight verification technique for testing whether a finite execution trace of a system satisfies a given specification. The specification is translated into a \emph{monitor}, which executes along with the monitored system: it consumes system events in a streaming manner and outputs the satisfaction or falsification of the specification. A widely used formalism for describing specifications for monitoring is \emph{Linear Temporal Logic} (LTL) \cite{havelund2004efficient}. Metric Temporal Logic (MTL) has been used for monitoring real-time temporal properties \cite{TR2005MTL}. Signal Temporal Logic (STL), which extends MTL with value comparisons, has been used for monitoring real-valued signals \cite{DDGJJS2017}.
Computing statistical aggregates of LTL-defined properties, as in \cite{finkbeiner2002collecting}, is a limited form of \emph{quantitative} monitoring.
The Eagle specification language \cite{barringer2004rule} can also express some quantitative monitoring properties, since it supports data-bindings.
% However, prior formalisms for monitoring either completely lack quantitative features or they do not allow a rich set of quantitative operations as we do here.

The line of work on synchronous languages \cite{BCEHlGdS2003SL} also deals with processing data streams. The focus in the design of these languages is the decomposition of the computation into logically concurrent tasks. Here, we focus on the control structure for parsing the input stream and applying quantitative aggregators.
Examples of synchronous languages designed for runtime monitoring include LOLA \cite{d2005lola} and its
extensions \cite{bozzelli2016foundations}.

%%%%%%%%%% DiffStream RW %%%%%%%%%%

We survey three broad categories of related work. First, we describe the state of the art on establishing correctness for data-parallel programs: we include both batch processing frameworks like MapReduce, and stream processing frameworks like Flink.
Second, we survey the use of partially ordered traces in modeling concurrency, and the associated problems that have been considered such as checking for linearizability, serializability, and data races.
Finally, we briefly mention the general testing methodologies our work fits under.

\section{Correctness for Data-Parallel Programs}

\paragraph{Testing.}
Many previous works focus on batch processing programs written in the MapReduce~\cite{MapReduce2008} framework \cite{csallner2011new,xu2013semantic,marynowski2012testing,chen2016commutativity} (see also the recent survey \cite{moran2019testing}). Going beyond batch processing, \cite{xu2013testing} study testing semantic properties of operators in general dataflow or stream-processing programs.
One problem with many of these works \cite{csallner2011new,xu2013semantic,xu2013testing,chen2016commutativity} is that real-world MapReduce programs (and, by extension, aggregators in stream processing programs) can be non-commutative: the empirical study at Microsoft~\cite{xiao2014nondeterminism} reports that about 58\% of 507 user-written reduce jobs are non-commutative, and that most of these are most likely not buggy. The previous work on testing would erroneously flag such programs as containing bugs due to nondeterminism, which would generate a large number of false positives. We adopt a black-box differential testing approach with the goal of avoiding this problem. Concretely, we have shown in \cref{ssec:evaluation-mapreduce} how to avoid a false positive for most cases where the application requirements imply that the nondeterminism is acceptable.

\paragraph{Static Verification.}
%
In addition to testing---a dynamic method of checking
correctness---there has also been research on the static verification
of data-parallel programs. Recent work focuses on the verification of
parallel aggregators that are used in MapReduce programs; either by
enabling automated verification and synthesis of \emph{partial
aggregators} given an aggregation function~\cite{liu2014automating},
or by parallelizing user defined aggregators using symbolic
execution~\cite{raychev2015parallelizing}. Both of these works help
developers by statically providing guarantees about the correctness of
parallel aggregator functions. DiffStream complements these approaches
by checking the correctness of general stateful streaming programs,
which are not always decomposable into aggregators, and whose parallel
and sequential implementations might have significant structural
differences (like the Topic Count case study in
\Cref{ssec:evaluation-wordcount}), implying that the parallel implementation cannot be simply
derived from the sequential implementation. Finally, another
difference is that DiffStream (and dynamic approaches in general) can be
used on programs that interact with external services (e.g. the Redis
database in \Cref{ssec:evaluation-wordcount}) without having to model
them---as is the case with static approaches.

\paragraph{Language Design.}
In contrast to the dynamic approach of testing and static verification techniques, there are language-based restrictions to achieve correct (i.e., semantics-preserving) parallelization in stream processing programs.
The language StreamIt~\cite{thies2002streamit} leverages Synchronous Dataflow~\cite{lee1987synchronous} to achieve correct parallelization; however, this requires a restriction on dataflow graphs where all operators must have a static \emph{selectivity} (number of output items produced per input item), so it is not appropriate for general stream processing where operators often lack static selectivity.
For general stream processing, \cite{schneider2013safe} and \cite{mamouras2019data} have proposed and implemented different approaches to ensure correct parallelization: the first is based on categorizing operators for properties such as statefulness and selectivity, while the second is based on a type discipline where streams are annotated with types. The idea of using dependence relations to specify partially-ordered output streams is originally proposed by \cite{mamouras2019data}, and here we apply that idea to testing and online monitoring.

\paragraph{Other Approaches.}
Complementary to directly establishing the correctness of user-written programs, there is work on indirectly facilitating correctness through visualization, through debugging, and finally by ensuring correctness at the system level.
Visualization includes generating example inputs for dataflow programs showcasing typical semantic behavior~\cite{olston2009generating}. Debugging includes, e.g., setting up breakpoints, stepping through computations, and determining crash culprits~\cite{gulzar2016bigdebug,olston2011inspector}.
Towards testing functional correctness of a stream processing system implementation, a framework has been proposed for Microsoft StreamInsight~\cite{raizman2010extensible}.

\section{Partially Ordered Traces in Concurrency Theory}

\paragraph{Mazurkiewicz Traces.}
We build on foundational work in concurrency theory dating back to Mazurkiewicz \cite{mazurkiewicz1986trace}, where partially ordered sets of events are called \emph{traces}. Mazurkiewicz traces have been studied from the
viewpoint of algebra, combinatorics, formal languages and automata, and
logic \cite{DiekertR1995}. In practical applications to verification and
testing of concurrent systems, they appear in relation to
\emph{partial order reduction}~\cite{God96,Peled94}, a technique for
pruning the search space of possible execution sequences.
The idea of a dependence relation to specify output ordering originally comes from Mazurkiewicz traces; however, the core algorithmic problem in our work corresponds to checking \emph{equivalence} of two Mazurkiewicz traces, and to our knowledge this particular testing problem has not been studied in any of the mentioned contexts.
An additional difference is that in the theory of Mazurkiewicz traces, one usually assumes a finite, symmetric, and reflexive dependence relation. In contrast, we only require it to be symmetric. This is in order to support user-provided dependence relations over a possibly infinite data domain, which is necessary to model common patterns in the streaming setting: one example is the key-based dependence relation (where the number of keys may be unbounded and different keys are independent). Patterns such as this one cannot be captured by a finite alphabet, and this limits the direct application of classical work on concurrency theory over a finite dependence relation, e.g.~\cite{DiekertR1995}.

\paragraph{Checking Properties of Traces.}
Much classical research has focused on deciding properties of traces such as serializability, linearizability, sequential consistency, and data race detection.
Broadly speaking, these properties are search problems: the algorithm monitors an execution of events, and it must decide if there exists some possible equivalent execution that witnesses the desired property. For example, race detection involves deciding, given a sequence of events, if there is a valid reordering of the events, subject to the constraints imposed by synchronization events, in which two specific events (representing a potential race condition) get reordered. The search aspect means that race detection is NP-hard~\cite{netzer1990complexity,netzer1992race}.
Similarly, checking sequential consistency of a given trace is NP-complete~\cite{gibbons1992complexity},
as is checking linearizability in general~\cite{gibbons1997testing}.
As a result, practical tools for testing correctness of traces (e.g., \cite{savage1997eraser,park2011efficient,sen2008race,wing1993testing,burckhardt2010line,lowe2017testing}) must explore the trade-off between soundness, completeness, and tractability.
In contrast, the problem we consider of checking two traces for equivalence up to re-ordering is in PTIME (for the offline variant), and admits an optimal online monitoring algorithm.

\section{Testing Methodologies}

\paragraph{Runtime Verification.}
Our work contributes to the large body of work on
runtime verification~\cite{leucker2009brief,havelund2004efficient}, a
lightweight verification paradigm which aims to identify bugs in the
output of a program as it is executed, using minimal computational
resources.  Most work in runtime verification focuses on detecting
violations of a property written in a logical specification language
(e.g., the temporal logic LTL and its extensions), whereas we consider
differential testing of a program against a reference implementation,
and we model program execution traces as partially rather than totally
ordered.

\paragraph{Differential Testing.}
Differential testing~\cite{mckeeman1998differential,groce2007randomized} is a well-established, lightweight, and scalable way to detect bugs in complex programs (for instance, in C compilers~\cite{yang2011finding}), by simply comparing two programs that are supposed to be equivalent.  We consider some specific problems that arise in the stream processing domain, specifically, output comparison in the presence of out-of-order data.

%% Cut RW Sections

% \paragraph{Safe Application Upgrade.}
% Our work is related to safe application upgrade, where the goal is to compare a trusted version of a program with an untrusted new version (see, for example~\cite{hosek2013safe,kim2012shared-execution,tucek2009delta-execution,maurer2012tachyon,hosek2015varan}); our algorithm in section~\ref{sec:algorithm} can be used for this purpose in the stream processing setting.

% \paragraph{Empirical Studies.}
% There are a number of empirical studies which aim to classify bugs in real-world stream- and batch-processing programs. Of these, most~\cite{schroeder2009large, kavulya2010analysis, li2013characteristic, zhou2015empirical} have primarily focused on sources of job failures (e.g., system crashes) or performance issues (e.g., memory use patterns and computational bottlenecks), which are orthogonal to semantic bugs which can be found by testing. The Microsoft study~\cite{xiao2014nondeterminism} is the only study we are aware of that classifies semantic bugs in user-written programs; this is why we used it in section~\cite{ssec:evaluation-mapreduce}.
% In addition to these studies of data-processing programs, there have been some empirical studies which interview users about their testing and debugging needs. In~\cite{fisher2012interactions}, users of Spark are interviewed about tools that would be useful to them, but the study focuses on \emph{human-computer interaction} needs such as data visualization and debugging tools. The more recent study~\cite{vianna2019exploratory} aims to determine how current specialists in data stream processing applications currently implement testing. Most specialist employ unit and integration testing, together with some techniques and tools for more sophisticated testing (e.g. simulating system failures). Our work is motivated by the need to go beyond these techniques which are standard in software engineering, to increase confidence in \emph{semantic correctness} of user-written programs, especially in the presence of parallelism and out-of-order data.
