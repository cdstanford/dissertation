\chapter{Related Work}
\label{cha:rw}

\headerblock{
  \headerquote{The sequence is represented by a function called a \emph{stream}, which is a functional analog of a coroutine ...
  % The conventional while and for loops of structured programming may be composed by a technique of stream processing (analogous to list processing), which results in more structured programs than the originals.
  This technique makes it possible to structure a program in a natural way into its logically separate parts, which can then be considered independently.}{William H. Burge, 1975~\cite{burge1975stream}}

  \headerbreak{}

  \headerquote{Stream processing research, in particular the study of SPSs, can be traced back at least as far as the 1960s, although not always in a form that is immediately recognizable as such today.}{Robert Stephens, 1997~\cite{stephens1997survey}}

  \headerbreak{}

  \headerquote{Of course, the notion of a stream as a programming abstraction has been around for decades...}{Thies, Karczmarek, and Amarasinghe, 2002~\cite{thies2002streamit}}
}

While the previous section discussed the primary programming paradigm in current use for distributed streams and its limitations, this section provides a brief more general survey of related work on programming paradigms, abstractions, and systems for modeling streaming data.

\section{Dataflow Programming}

\subsection{Distributed Stream Processing Systems}

Applications over streaming data can be implemented using
high-performance, fault tolerant distributed stream processing systems, such as
Apache
Flink \cite{Flink2015,Flink2017,Flink},
Storm~\cite{Storm},
Spark Streaming~\cite{Spark2013,SparkStreaming},
Kafka~\cite{garg2013apache},
Samza~\cite{Samza2017},
Heron~\cite{kulkarni2015twitter-heron,Heron},
and Beam~\cite{Beam};
Timely Dataflow (Naiad)~\cite{Naiad2013,Timely}
and
Differential Dataflow~\cite{mcsherry2013differential};
Microsoft
Trill~\cite{chandramouli2014trill};
IBM SPL~\cite{HAG2013SPL};
Google MillWheel~\cite{MillWheel} (now replaced by Google Cloud Dataflow);
Amazon Kinesis~\cite{AmazonKinesis};
and early systems such as
Aurora~\cite{Aurora,AuroraWeb},
Boreals~\cite{Borealis,BorealisWeb},
STREAM~\cite{STREAM2004},
and TelegraphCQ~\cite{Telegraph}.
The core programming model for these systems is typically based on dataflow with autoparallelization, as discussed in~\Cref{cha:background},
and this is the primary point of comparison for this thesis.
However, there are other programming models used for streaming, including
high-level query languages (often based on SQL),
extensions to the dataflow model,
and extensions to stream parallelism and distribution.

\subsection{High-level Query Languages}

High-level query languages for streaming include Streaming SQL~\cite{jain2008towards,begoli2019one}, CQL~\cite{arasu2003cql,ABW2006CQL}, SamzaSQL~\cite{pathirage2016samzasql}, Structured Streaming~\cite{armbrust2018structured},
and StreamQRE~\cite{StreamQRE}.
High-level query languages have existed since the early history
of streaming research from the databases community~\cite{stonebraker20058}.
They offer the promise of convenience and clean semantics, but they can be expressively limited for some use cases (e.g., the value-barrier example discussed in the introduction), as we discuss in~\citeMain{ppopp22}.
They typically offer type-safety with respect to the relational schema of each stream and determinism with respect to processing order --
though there are cases where determinism is not always guaranteed, including in some possible implementations of CQL's tuple-based windows where ties must be broken arbitrarily.\footnote{Thank you to Phillip Hilliard for this observation.}
Traditional query languages view streams in the \emph{sequence-of-relations} model popularized by CQL, which is limited in expressiveness compared to general partial orders.
Tuple-based windows are an example of where this limit is evident: there is no way to have a stream which is a true sequence of events at the type level, as there is always the possibility that multiple events could arrive concurrently at the same timestamp.
% Joins can also be highly expensive, and in the worst case, unbounded as there is no guarantee that a join can be parallelized and optimized effectively.

In modern systems, query operators (typically including maps, windows, filters, joins, and aggregates)
are often implemented not as a separate stream management platform but as
operators on top of the core dataflow programming model, which can also support custom stateful processing and where data need not conform to the relational schema viewpoint.
The upshot of this two-layered design is that the parallelism present at the core dataflow programming model level is highly relevant even for query languages as: (i) it dictates the extent to which streams can be distributed and optimized, and (ii) it forces semantic requirements on the query in case determinism is required, as otherwise distribution will not be semantics-preserving.

\subsection{Traditional Dataflow Networks and Synchronous Languages}

Dataflow programming predates stream processing.
Dataflow programming as a solution to parallel and distributed computing
dates back to Kahn Process Networks~\cite{gilles1974semantics} (KPN), a deterministic dataflow model based on the restriction that channels are FIFO queues with blocking reads and non-blocking writes. Today's systems wish to offer on-demand processing and avoid arbitrary buffering, so they typically do not implement the KPN approach for streams.
In addition to general KPNs, one restriction of KPNs has been particularly influential: synchronous dataflow~\cite{lee1987synchronous}, which further restricts the FIFO queues so that a fixed number of items are read and written on each cycle of an operator. StreamIt~\cite{thies2002streamit} shows that this leads to massive opportunities in optimization and scheduling, and it in a sense eliminates the problem with blocking reads because the presence or absence of input and output items is always known statically.

Similar to StreamIt,
the work on synchronous languages from the 90s including LUSTRE~\cite{halbwachs1991synchronous} and ESTEREL~\cite{berry1992esterel} (see~\cite{BCEHlGdS2003SL} for an overview) benefits from the assumption of
synchrony.
The problem with traditional synchronous dataflow languages is that they cannot implement operators which produce an non-static number of output items in response to an input -- including the very most basic such operator, \emph{filter}. Filter of an input stream discards items that do not match a predicate. As a result, the synchronous model is generally considered to restrictive today for general streaming~\cite{schneider2013safe}.

\subsection{Modern Dataflow Languages}

A more modern take on dataflow programming is MapReduce online~\cite{condie2010mapreduce}; this exemplifies the viewpoint that streaming dataflow graphs are like MapReduce operators chained together.
Other works on streaming specifically focusing on the programming model include SPADE~\cite{gedik2008spade} and Brooklet~\cite{soule2010universal}.
SPADE supports a fixed list of useful operators for streams, like tuple-level transformations (map, flat-map, filter), aggregations, and streaming joins or barriers; it also incorporates some punctuation-related operators.
Brooklet is more like imperative programming,
and is extremely general: it allows translations from CQL, StreamIt, and a (non-streaming) MapReduce like language called SawZall~\cite{pike2005interpreting}.
Both of these languages have coarse-grained type systems;
i.e., they don't expose finge-grained parallelism information in stream types.

\subsection{Extensions to the Dataflow Model}

The dataflow model is limited in its ability to express iterative or recursive queries and other queries with periodic synchronization between nodes, leading to various extensions focused on better expressiveness.
Naiad~\cite{Naiad2013,Timely} proposes \emph{timely dataflow} in order
to support iterative computation.
We compare closely with the implementation of timely dataflow in Rust in \Cref{cha:distribution}.
In brief, though Timely is very expressive it is also often quite low-level; as a result, it falls short of automatically scaling without high-level design sacrifices (exposing implementation details to the user).
Timely also assumes that all events are unordered, and thus requires unnecessary buffering, complicating the programming model in cases where order would be known between events as our type system allows.

As data processing applications are becoming more complex, evolving from
data analytics to general event-driven applications, some stream
processing and database systems are moving from dataflow programming to more
general actor models
\cite{CarboneFKK20,Bernstein19,BernsteinDKM17,Das2018,xu2021move}.
For example, Flink has recently released Stateful Functions,
an actor-based programming model running on top of Flink
\cite{AkhterFK19,StatefulFunctions}.
Actor models are very expressive, but computations need to be implemented manually as
message-passing protocols.

Other extensions to the dataflow model focus on enabling forms of synchronization between nodes or other concurrency control, or communication with external state.
Communication between parallel nodes is disallowed in the usual formulation of dataflow autoparallelization.
For example, S-Store and TSpoon~\cite{meehan2015s,affetti2020tspoon} extend stream processing systems with online transaction processing (OLTP), which can implement concurrency control,
and Nova~\cite{zhao2021timestamped} extends stream processing systems with a shared state abstraction.
Another way to deal with the problem of communication between nodes
is to use \emph{broadcast state}, a low-level messaging mechanism present in
Timely~\cite{BroadcastStateTimely} and Flink~\cite{BroadcastStateFlink}
which lets one node broadcast a message to all other parallel nodes.
We discuss our alternative solution to all of these issues related to concurrency and synchronization in \Cref{cha:distribution}.
In brief, our model avoids sacrificing high-level design by making the distribution of a program independent of the programming model, and guarantees that assuming the program is consistent, the implementation is deterministic.

\subsection{Summary}

In all of the existing works on dataflow surveyed, streams are typed at a coarse-grained level: typically, only using a construct such as \texttt{Stream<T>} for a stream of events of type \texttt{T}, and not encoding the possible parallelization.
Some languages include a few variants, such as Flink's \texttt{KeyedStream} which is parallelized by key, and CQL's distinction between streams \texttt{Stream<T>} and time-series elations \texttt{Relation<T>} which are created from streams using windowing operators.
Compared to coarse-grained stream types, fine-grained stream types can be seen as a way to record parallelization information across the software stack, whether at the system level, the dataflow model level, or at the query language level.
For example, in \Cref{cha:distribution} we pass types from the distributed programming model to the distributed runtime and use them to implement automatic safe distribution.
The original presentation of \Cref{cha:foundation}'s type system in~\citeMain{pods21} was inspired by database schema types at the query language level.

\section{Correctness Support for Data-Parallel Programs}

\subsection{Testing}

Many previous works focus on batch processing programs written in the MapReduce~\cite{dean2008mapreduce} framework \cite{csallner2011new,xu2013semantic,marynowski2012testing,chen2016commutativity} (see also the recent survey \cite{moran2019testing}). Going beyond batch processing, \cite{xu2013testing} studies testing semantic properties of operators in general dataflow or stream-processing programs.
One limitation of many of these works \cite{csallner2011new,xu2013semantic,xu2013testing,chen2016commutativity} is that real-world MapReduce programs (and, by extension, aggregators in stream processing programs) can be non-commutative: the empirical study at Microsoft~\cite{xiao2014nondeterminism} reports that about 58\% of 507 user-written reduce jobs are non-commutative, and that most of these are most likely not buggy. The previous work on testing would erroneously flag such programs as containing bugs due to nondeterminism, which would generate a large number of false positives.

Differential testing~\cite{mckeeman1998differential,groce2007randomized} is a well-established, lightweight, black-box method to detect bugs in complex programs by simply comparing two programs that are supposed to be equivalent.
In \Cref{cha:testing}, we adopt differential testing to finding bugs due to parallelism with the goal of avoiding the false-positives mentioned in the previous paragraph; we do succeed in avoiding false positives in most cases, though not in a few others where nondeterminism is truly inherent to the computation.

\subsection{Static Verification}

In addition to testing---a dynamic method of checking
correctness---there has also been research on the static verification
of data-parallel programs. Recent work focuses on the verification of
parallel aggregators that are used in MapReduce programs; methods include automated verification and synthesis of \emph{partial
aggregators} given an aggregation function~\cite{liu2014automating},
or by parallelizing user defined aggregators using symbolic
execution~\cite{raychev2015parallelizing}. Both of these works help
developers by statically providing guarantees about the correctness of
parallel aggregator functions.

We would be interested if similar ideas could be generalized to the abstractions in this thesis,
rather than just MapReduce programs;
i.e. we'd like to verify and synthesize operators in streaming dataflow graphs.
Note that streaming graphs are not always decomposable into aggregators, and their parallel
and sequential implementations might have significant structural
differences (see the Topic Count case study in
\Cref{cha:testing}), implying that the parallel implementation cannot be simply
derived from the sequential implementation.

For general stream processing, \cite{schneider2013safe} has proposed an approach to ensure correct parallelization (deterministic distribution) based on categorizing operators for properties such as statefulness and selectivity.
This is very closely related to our work on guaranteeing type-safe, deterministic distribution;
while it is practical, it could be considered ad hoc to enumerate operators into finitely many categories.
Another complication is if considering streaming graphs that interact with external service (e.g. querying a Redis database) or complex extensions to the dataflow model including operators like broadcast-state.

Instead of verifying user-written streaming programs, one can instead consider the problem of correctness for systems, compilers, and implementations.
Towards testing functional correctness of a stream processing system implementation, a framework has been proposed for Microsoft StreamInsight~\cite{raizman2010extensible}.

Overall, verifying streaming dataflows statically is an important problem for future work which is not yet easily within our reach, and which we discuss further in \Cref{cha:discussion}.

\subsection{Empirical Studies and Debugging}

Complementary to directly establishing the correctness of user-written programs, one can look at the problem of correctness from an empirical and engineering perspective.
There are a number of empirical studies which aim to classify bugs in real-world stream- and batch-processing programs. Of these, most~\cite{schroeder2009large, kavulya2010analysis, li2013characteristic, zhou2015empirical} have primarily focused on sources of job failures (e.g., system crashes) or performance issues (e.g., memory use patterns and computational bottlenecks), which are orthogonal to semantic bugs which can be found by testing. The Microsoft study~\cite{xiao2014nondeterminism} is the only study we are aware of that classifies semantic bugs in user-written programs.
In addition to these studies of data-processing programs, there have been some empirical studies which interview users about their testing and debugging needs. In~\cite{fisher2012interactions}, users of Spark are interviewed about tools that would be useful to them, but the study focuses on \emph{human-computer interaction} needs such as data visualization and debugging tools. The more recent study~\cite{vianna2019exploratory} aims to determine how current specialists in data stream processing applications currently implement testing. Most specialist employ unit and integration testing, together with some techniques and tools for more sophisticated testing (e.g. simulating system failures). Our work is motivated by the need to go beyond these techniques which are standard in software engineering, to increase confidence in \emph{semantic correctness} of user-written programs, especially in the presence of parallelism and out-of-order data.

These empirical studies are an important motivation for facilitating correctness through techniques such as visualization and debugging.
Visualization includes generating example inputs for dataflow programs showcasing typical semantic behavior~\cite{olston2009generating}. Debugging includes, e.g., setting up breakpoints, stepping through computations, and determining crash culprits~\cite{gulzar2016bigdebug,olston2011inspector}.

\section{Partially Ordered Trace Theory}

\subsection{Mazurkiewicz Traces}

Our type system builds on foundational work in concurrency theory dating back to Mazurkiewicz \cite{mazurkiewicz1986trace}, where partially ordered sets of events are called \emph{Mazurkiewicz traces}. Mazurkiewicz traces have been studied from the viewpoints of algebra, combinatorics, formal languages and automata, and
logic \cite{DiekertR1995}. In practical applications to verification and
testing of concurrent systems, they appear in relation to
\emph{partial order reduction}~\cite{God96,Peled94}, a technique for
pruning the search space of possible execution sequences.

Mazurkiewicz traces correspond to the view of streams as linearizations and as labeled partially ordered sets (see \Cref{cha:foundation}).
Both of these views and the isomorphism between them are standard and well-known in this literature.
Our type system, however, is an abstraction on top of Mazurkiewicz traces; it
gives rise not to all dependence relations, but only to certain ones that have a series-parallel structure.
See \Cref{prop:stream-types-less-general}
We believe that the series-parallel streams constitute most useful use cases in practice, where there are usually only perhaps one or two levels of nesting in the type. Most streams consist of events of maybe one or two base types, with punctuation and other system events, and our types aim to cater towards these simpler use cases.

An important technical difference is that in the theory of Mazurkiewicz traces, one usually assumes a finite, symmetric, and reflexive dependence relation~\cite{DiekertR1995}. In contrast, in this thesis, we only require it to be symmetric; it is neither finite (due to arbitrary infinite base-types and key-based parallelism) nor reflexive (due to the relational base type). This is in order to support user-provided dependence relations over a possibly infinite data domain, which is necessary to model common patterns in the streaming setting. Patterns such as this one cannot be captured by a finite alphabet, and this limits the direct application of classical work on concurrency theory over a finite dependence relation.

\subsection{Checking Properties of Traces}

Much classical research has focused on deciding properties of traces such as serializability, linearizability, sequential consistency, and data race detection.
Broadly speaking, these properties are search problems: the algorithm monitors an execution of events, and it must decide if there exists some possible equivalent execution that witnesses the desired property. For example, race detection involves deciding, given a sequence of events, if there is a valid reordering of the events, subject to the constraints imposed by synchronization events, in which two specific events (representing a potential race condition) get reordered. If there are an arbitrary number of threads then race detection is NP-hard~\cite{netzer1990complexity,netzer1992race}
(but it is easy to decide for, say, only two traces and can be done in a streaming manner).
Similarly, checking sequential consistency of a given trace is NP-complete~\cite{gibbons1992complexity},
as is checking linearizability in general~\cite{gibbons1997testing}.
Practical tools for testing correctness of traces (e.g., \cite{savage1997eraser,park2011efficient,sen2008race,wing1993testing,burckhardt2010line,lowe2017testing}) are bound by these results and explore the trade-off between soundness, completeness, and tractability.

In our work we don't generally consider algorithmic problems on traces,
as we are focused on language design.
The exception to this is \Cref{cha:testing},
where we consider the algorithmic problem of checking \emph{equivalence} of two Mazurkiewicz traces (equality of streams up to re-ordering).
As with our type system, we consider this problem for general infinite-alphabet traces, rather than just those over a finite alphabet.
The problem we consider is in PTIME (for the offline variant), and admits an optimal online monitoring algorithm.
Probably because the onlien monitoring algorithm doesn't have a good worst-case complexity bound, to our knowledge it hasn't been explicitly studied and articulated in existing work on Mazurkiewicz traces.

\section{Other Paradigms}

\subsection{Concurrent and Parallel Programming Models}

Fork-join based
concurrent programming~\cite{frigo1998implementation,lea2000java}
constitutes a classical parallel programming paradigm.
There is a vast body of work on parallel programming models based on forks and joins, but it is typically low-level and does not guarantee determinism or data-race freedom.
Many proposals exist for making concurrent programming safe (typically not fully deterministic, but at least data-race free).
For example, fearless concurrency~\cite{milano2022flexible} is a recent proposal for concurrent programming which guarantees safe execution via a static type system.

\subsection{Distributed Programming Models and Consistency}

Concurrent Revisions~\cite{burckhardt2010concurrent}
guarantees determinism in the presence of concurrent updates by allowing programmers to specify isolation types that are processed in parallel and then merged at join points.
Another related domain is monotonic lattice-based programming models,
including
Conflict-Free Replicated Data Types~\cite{shapiro2011conflict},
Bloom$^L$~\cite{conway12},
and LVars~\cite{lvars13,lvars14},
which are designed for coordination-free distributed programming.
These models guarantee strong eventual consistency,
i.e., eventually all replicas will have the same state,
but, in contrast to our model, CRDTs and Bloom$^L$
do not allow synchronization between different workers.
LVars, which focuses on determinism for concurrent updates on shared variables,
extends lattice-based models with a freeze operation that enforces a synchronization point,
inducing partial order executions that are similar to the ones in our model.
Consistency for replicated data stores
often relies on a form of dependence relation between events.
Some examples include RedBlue consistency~\cite{li2012making},
MixT~\cite{milano2018mixt},
Quelea~\cite{sivaramakrishnan2015declarative},
CISE~\cite{gotsman16},
Carol~\cite{lewchenko2019sequential},
all of which support a mix of consistency guarantees on different operations,
inducing a partial order of data store operations.

\subsection{State Machines for Data Processing}

\Cref{cha:monitoring} proposes a state machine representation for performance-sensitive data stream processing;
the core expressiveness problem is to enable \emph{quantitative} computations that aren't necessarily finite-state.
More generally, there are many proposals to generalize finite state machines and classical automata theory to more general data processing applications.

\emph{Weighted automata}, which were introduced in \cite{S1961WA} (see also the more recent monograph \cite{DKV2009HWA}), extend nondeterministic finite-state automata by annotating transitions with \emph{weights} (which are elements of a semiring) and can be used for the computation of simple quantitative properties. A weighted automaton maps an input string $w$ to the minimum over costs of all accepting paths of the automaton over $w$.
Extensions such as \emph{nested weighted automata} \cite{CHO2015NWA} enjoy increased expressiveness, but fall short of capturing an arbitrary set of data types and operations as CRAs and DTs do. We studied arbitrary hierarchical nesting of weighted automata in \citeMain{icalp17}, which does allow arbitrary types and operations. We showed that under certain typing restrictions there is a streaming evaluation algorithm.
Our model in~\Cref{cha:monitoring} admits streaming evaluation \emph{sans} typing restrictions; it is ``flat'', i.e. not recursively defined; the transition structure makes modular composition feasible; and we have clean succinctness results.

Another approach to augment classical automata with quantitative features has been with the addition of \emph{registers} that can store values from a potentially infinite set. These models are typically varied in two aspects: by the choice of data types and operations that are allowed for register manipulation, and by the ability to perform tests on the registers for control flow.
The literature on data words, data/register automata and their associated logics \cite{KF1994FMA, NSV2004FSM, DL2009LFQ, BS2010NRDL, BDMSS2011LDW} studies models that operate on words over an infinite alphabet, which is typically of the form $\Sigma \times \mathbb{N}$, where $\Sigma$ is a finite set of tags and $\mathbb{N}$ is the set of the natural numbers. They allow comparing data values for equality, and these equality tests can affect the control flow.
Monitors with finite-state control and unbounded integer registers are also studied in \cite{FHS2018} and a hierarchy of expressiveness is established on the basis of the number of available registers.
The work on cost register automata (CRAs) \cite{AdADRY2013CRA, AR2013ARF} and streaming transducers \cite{AC2010SST, AC2011STA, AdA2012STT} is about models where the control and data registers are kept separate by allowing write access to the registers but no testing.

As discussed in \Cref{cha:monitoring}, \Cref{dt:subsec:dts-and-cras},
data transducers are expressively equivalent to CRAs, so they belong to the same class of models. However, they are exponentially more succinct.

Expressively and in logical terms, both CRAs an DTs recognize the class of \emph{streamable regular transductions}~\citeMain{tcs20}.
This class can also be defined by monadic second order logic (MSO) or attribute grammars.

\subsection{Runtime Verification and Monitoring}

Our work in \Cref{cha:testing} and \Cref{cha:monitoring} contribute to the large body of work on runtime verification~\cite{leucker2009brief,havelund2004efficient} (also known as \emph{runtime monitoring}), a
lightweight verification paradigm which aims to identify bugs in the
output of a program as it is executed, using minimal computational
resources.
(The testing problem we consider is a runtime verification problem, and
the data transducers work is a language that can be used for monitoring applications.)
In typical work on runtime verification, the problem is to detect
violations of a property written in a logical specification language.
The specification is translated into a \emph{monitor}, which executes along with the monitored system: it consumes system events in a streaming manner and outputs the satisfaction or falsification of the specification. \emph{Linear Temporal Logic} (LTL) is the most widely used formalism for describing specifications for monitoring, including many extensions for various tasks.

In particular, relevant to \Cref{cha:monitoring}, there are some \emph{quantitative} extensions to monitoring formalisms.
Metric Temporal Logic (MTL) has been used for monitoring real-time temporal properties \cite{TR2005MTL}. Signal Temporal Logic (STL), which extends MTL with value comparisons, has been used for monitoring real-valued signals \cite{DDGJJS2017}.
Computing statistical aggregates of LTL-defined properties, as in \cite{finkbeiner2002collecting}, is a limited form of quantitative moitoring.
The Eagle specification language \cite{barringer2004rule} can also express some quantitative monitoring properties, since it supports data-bindings.
Finally, the synchronous languages~\cite{BCEHlGdS2003SL} mentioned earlier can also be used for monitoring quantitative data streams.
LOLA \cite{d2005lola,bozzelli2016foundations} is a notable example of a synchronous language designed for runtime monitoring
with close similarity to state-machine based monitors.
RTLola~\cite{faymonville2017real} extends LOLA to the real-time monitoring setting, rather than synchronous monitoring.

In contrast to classical work in runtime verification and monitoring, our core type system in this thesis models program execution traces as partially rather than totally ordered; our ultimate goal is runtime verification abstractions which work on partially ordered streams, for which \Cref{cha:testing} is an initial proposal, handling the simplest case of equality checks between streams.

% \subsection{Reactive Programming}
% TODO WRITE THIS
% FRP

% \subsection{Programmable Networks}
% TODO WRITE THIS

\section{Some Systems Challenges}

On the other side of the programming model,
researchers have proposed distribution, parallelization, and optimization strategies for stream programs.
These can be relevant to the programming model as they affect the sort of semantic guarantees that the system can offer with respect to parallelism;
however, the primary goal of these works is to enable efficient system implementations.

The literature in this area is vast; we survey a small selection of papers focused on those which try to do distributed optimization, which is relevant for \emph{geo-distributed stream processing}~\citeMain{wpe2}. The geo-distributed streaming problem is in particular to enable streaming applications over many distributed processes or nodes that don't necessarily all reside in the same central server or cluster.

\subsection{Operator Placement}

A fundamental problem in distribution of streaming operators, and closely related to the distribution problem considered in \Cref{cha:distribution},
is \emph{operator placement} where the system determines what node to run a stream operator on.
The work~\cite{cardellini2016optimal} uses constraint solving to optimize operator placement relative to network bandwidth and other constraints.
Other than~\cite{cardellini2016optimal}, there are several lines of work in job scheduling, operator placement, and optimization for DSPS that try to be network-aware in some fashion. Early and influential works include~\cite{ahmad2004network} and~\cite{pietzuch2006network}.
The first~\cite{ahmad2004network} is probably the first to formalize the operator placement problem for DSPS, and to explore (1) how network-awareness can lead to more efficient query evaluation, and (2) how there is a trade-off between latency and bandwidth use in this space.
The second~\cite{pietzuch2006network} presents a simple but effective heuristic algorithm which treats the geo-distributed physical nodes as a system of points in a combined latency-bandwidth space, and uses spring relaxation find a good configuration.
The work~\cite{gu2015general}, similarly to~\cite{cardellini2016optimal}, formulates the problem in MILP.
There are a number of other related papers on job scheduling~\cite{aniello2013adaptive,xu2014tstorm,eidenbenz2016task,wolf2008soda,fu2019edgewise}, operator placement~\cite{bonfils2004adaptive,tziritas2016improving,rizou2010solving,lakshmanan2008placement},
and resource elasticity~\cite{cardellini2018decentralized,hochreiner2016elastic,cardellini2018optimal,dias2018dsp-survey}.

Researchers have also modified the programming model to allow the programmer to control operator placement.
SpanEdge~\cite{sajjad2016spanedge} is a primitive modification to the dataflow programming model with tasks that should be run globally versus locally.
A system very related to SpanEdge is Geelytics~\cite{cheng2015geelytics}.
It modifies the dataflow programming model with \emph{scoped tasks}, which have a geographic granularity such as by site, by city, by district, or by section, and are similar to SpanEdge's local and global tasks.
Other than these, the paper~\cite{renart2017datadrivenstreamedge} proposes a programming framework for stream processing in a geo-distributed (at the edge) fashion. The programming framework, however, is not based on dataflow, and is more focused on the communication mechanisms between nodes. There is also a large body of work on programming for wireless sensor networks; see the survey~\cite{mottola2011programming-wsn}. In general, the concerns in that domain have been more low-level, related to connections between sensors, mobility of sensors, communication from one sensor to another via short hops, and so on.

\subsection{Stream Degredation}

An even more aggressive technique for optimizing distributed execution is to degrade and approximate streams to reduce what needs to be sent over the network.
The first stream processing system to incorporate general degradation of data streams was JetStream~\cite{rabkin2014jetstream}.
JetStream seeks to limit bandwidth use not just through degradation, but also ``aggregation'', which refers to a data model where data is saved and aggregated by geo-distributed nodes, and only sent when explicitly requested.
AWStream~\cite{zhang2018awstream} uses programming knobs to control the amount of degredation that occurs for video streams (e.g. frame rate reduction, resolution reduction, or some combination) to try to achieve a Pareto-optimal solution between accuracy and bandwidth use.

\emph{Load shedding}~\cite{tatbul2003load,tatbul2007staying} can be seen as a primitive form of stream degredation.
This refers to selectively dropping tuples in response to load that is beyond capacity, in order to maintain availability and good latency while hopefully not losing too much accuracy.
For video data, for instance, load shedding would enable frame rate reduction but would not allow resolution reduction. It is well studied, but less flexible than what is offered by more modern systems like JetStream and AWStream.
