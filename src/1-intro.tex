\sectionplain{Introduction}
\label{sec:intro}

% \meaning\section

Today, data is produced at an overwhelming rate
that cannot be processed by traditional methods.
For example, Cisco has estimated in its annual white paper
that data produced by people, machines, and things
is around 500 zettabytes, in contrast to a much smaller volume
of data that can be feasibly stored~\cite{index2018forecast}.
Reachers and industry practice have accordingly recognized the demand
for a new paradigm of computing where data is
distributed (processed in parallel over many devices),
transient (processed as it arrives and discarded),
and temporally structured (considered with respect to time).
This \emph{stream processing} paradigm has given rise to an increasing number
of modern data processing software platforms\footnote{E.g.: Kinesis from Amazon, Timely and Differential Dataflow from Materialize, and a range of open-source frameworks including Spark, Storm, Flink, Kafka, Samza, Heron, and Calcite from the Apache Software Foundation.}.
Broadly construed, the stream processing paradigm is also recognized by designers of other modern systems: these include microservices deployed in the cloud; IoT and other edge devices, which operate in response
to sensor data~\cite{shi2016edge, ashton2009internet}; and even programmable network switches,
which aim to push some of this expensive streaming computation
into the network.
Stream processing also has theoretical justification in the \emph{streaming model of computation},
where items arrive one at a time and are processed as they arrive, using
an ideally minimal amount of space and time per element.
Despite the widespread interest, researchers have devoted inadequate attention to software correctness and safety~\citeMain{stanford2022correctness}.
Herein, we investigate the stream processing paradigm from the lens of \emph{programming languages}, and provide software abstractions which are type-safe, deterministic, and performant: that is, which save programmers from common mistakes and ensure that the program that is deployed meets their intent.

What is a stream?

In this thesis, we address these limitations by providing abstractions and tools to make programming DSPS more reliable.
To put DSPS on formal foundations, we first propose \emph{partial-order semantics} for data streams.
We argue that events in the system should be viewed as a partially ordered set,
where events at different nodes should be viewed as unordered if their order
is not guaranteed by the system.
The partial order viewpoint, called \emph{dependence relations} and inspired by foundational work in concurrency theory,
forms a common semantic framework for much of the work in this thesis.
We define two typing disciplines for partially ordered streams:
data-trace types~\citeMain{festschrift18,pldi19},
and synchronization schemas~\citeMain{pods21}.
In addition to static typing guarantees~\citeMain{pldi19},
we use the partial order viewpoint for differential testing~\citeMain{oopsla20}
and for to enable \emph{safe} distribution, i.e. distribution with correctness guarantees~\citeMain{ppopp22}.
The motivation for the former is that while static types are useful
for new applications, existing applications are not developed with the new
framework and so can benefit from runtime testing to identify bugs due to
output even reordering.
The motivation for the latter work is to define a parallel programming framework appropriate to DSPS where distribution is semantics-preserving
rather than potentially semantics-breaking.

%% Maybe useful from research statement
% If successful, new programming languages and formal methods research could revolutionize online systems development.
% This research could enable applications that are more semantically robust, bug-free, and verified both at development time and during live deployment.
% On the technical side, there are a wealth of research problems to tackle that require combining new ideas and understanding with existing paradigms from data management, distributed systems, networks, programming languages, and algorithms.

In the second part of the thesis, we aim to offer provable guarantees about performance: in particular considering finite-state models of stream processing systems~\citeMain{icalp17,popl19,tcs20}.
In particular, we target \emph{runtime monitoring} applications
over data streams, and we show how to define and compile runtime monitors
with provable performance.
These can be used either to monitor DSPS applications on the side,
or to define DSPS operators themselves.
While the existing work has been used for compilation of high-level query languages on a single machine with space and time bounds~\citeMain{popl19}~\cite{QRE,StreamQRE},
the primary direction for future work here is to adapt to the \emph{distributed setting} and show similar performance guarantees there.

For the remaining future work, we target an implementation of the synchronization schemas~\citeMain{pods21} and data transducer~\citeMain{popl19} abstractions on top of Timely Dataflow~\cite{Timely,Naiad2013} in Rust~\cite{RustLang}.
Timely is a good choice because it offers a semantically sound low-level dataflow representation,
and we aim to leverage Rust's type system for compile-time guarantees,
while generating external verification conditions to prove user programs correct.

\subsection{Contributions}

In summary, we make the following (completed and planned) contributions:

\begin{itemize}
\item
\emph{Semantics:}
We propose \emph{partial order semantics} for DSPS,
through the data-trace types~\citeMain{pldi19} and synchronization schemas~\citeMain{pods21} typing disciplines.
We show how viewing streams as partially ordered allows for deterministic semantics,
and show how well-typed programming disciplines can be developed,
leading to well-defined DSPS programming libraries.
(\Cref{sec:semantics})

\item
\emph{Testing:}
To detect bugs due to nondeterminism in existing DSPS applications,
we propose DiffStream, a differential testing framework~\citeMain{oopsla20}.
In particular, we leverage the partial order viewpoint to specify
ordering requirements, and we test for violations at runtime.
(\Cref{sec:testing})

\item
\emph{Safe distribution:}
For more complex application logic where currently error-prone
low-level state management is used,
we propose \emph{dependency-guided synchronization},
a programming framework for \emph{semantics-preserving} distribution~\citeMain{ppopp22}.
A key aspect of this work is the ability to program
synchronization between distributed nodes, which is not supported
by most existing systems,
while still ensuring safety and allowing automatic distribution
of streaming applications.
(\Cref{sec:distribution})

\item
\emph{Monitoring:}
Towards streaming applications with predictable performance,
we propose data transducers, a monitoring formalism and state-machine based intermediate representation~\citeMain{popl19}.
Our formalism is compositional, enabling compilation of high-level
monitoring queries with provable performance bounds.
(\Cref{sec:monitoring})

\item
\emph{Verification:}
As future work, we propose a verified streaming library
over partially ordered streams, incorporating ideas from synchronization schemas~\citeMain{pods21} and data transducers~\citeMain{popl19}.
The library enables a high-level query language which is well-typed,
where typing properties are checked partly statically and partly by
generating external verification conditions.
(Section~\ref{sec:fw})
\end{itemize}
