\chapter{Introduction}
\label{cha:intro}

% Today, data is generated with higher velocity and higher volume than can be feasibly stored, raising the need for new algorithms, software abstractions, and systems.
Today, data is produced at an overwhelming rate
that cannot be processed by traditional methods.
For example, Cisco has estimated in its annual white paper
that data produced by people, machines, and things
is around 500 zettabytes, in contrast to a much smaller volume
of data that can be feasibly stored~\cite{index2018forecast}.
Researchers and industry practice have accordingly recognized the demand
for a new paradigm of computing where data is
distributed (processed in parallel over many devices),
transient (processed as it arrives and discarded),
and temporally structured (considered with respect to time).
This \emph{stream processing} paradigm has given rise to an increasing number
of modern data processing software platforms.\footnote{E.g.: Kinesis from Amazon, Timely and Differential Dataflow from Materialize, and a range of open-source frameworks including Spark, Storm, Flink, Kafka, Samza, Heron, and Calcite from the Apache Software Foundation.}
% TODO Cite
Broadly construed, the stream processing paradigm is also exemplified not just by dedicated stream processing software frameworks, but by many other modern systems: these include microservices deployed in the cloud; IoT and other edge devices, which operate in response
to sensor data~\cite{shi2016edge, ashton2009internet}; and even programmable network switches,
which aim to push some of this expensive streaming computation
into the network.
Stream processing also has theoretical justification in the \emph{streaming model of computation},
where items arrive one at a time and are processed as they arrive, using
an ideally minimal amount of space and time per element.
In this thesis, we investigate the stream processing paradigm from the lens of \emph{programming languages}, by investigating software abstractions which are type-safe, deterministic, and performant: that is, which save programmers from common mistakes and ensure that the program that is deployed meets their intent.

\section{Motivation}

Although research in stream processing can be traced back two decades from within the database community~\cite{Aurora,Borealis,STREAM2004,ABW2006CQL},
researchers have devoted inadequate attention to software correctness (see the short paper~\citeMain{stanford2022correctness} for an overview of our viewpoint).
Today's systems are difficult to test for correctness and debug. In the context of Apache Spark, researchers found that bugs are very difficult to diagnose due to a number of issues related to distributed deployment (e.g., a bug only shows up on a particular input item out of millions, in a particular distributed execution, or in the presence of faults)~\cite{gulzar2016bigdebug}. For streaming applications in Apache Flink, user studies have demonstrated that the state of practice is unit and integration testing~\cite{vianna2019exploratory}.
For complex use cases involving any amount of custom logic, building and deploying a correct application in today's systems often requires either significant expertise in distributed systems or a good deal of experience with developing and debugging for the specific streaming framework in question. We posit that there is therefore both a need and a research opportunity for more sophisticated testing and formal correctness techniques. With the right abstractions and automated formal tools, programming correct distributed applications over data streams could be much easier for inexperienced users than it is today.

From a programming languages viewpoint, two problems stand out in particular. First, existing systems lack \emph{safe semantics}. We will ellaborate further on what we mean by safety more when describing our proposed model -- in brief, existing systems lack \emph{type safety} with respect to fine-grained differences in streams and how they are parallelized, and lack \emph{determinism} with respect to all possible parallel or distributed executions. In the rest of the thesis, each section can be seen as focusing on a particular kind of formal safety property for programming over streams.

Second, existing systems disagree on basic details about how streams are parallelized. Points of disagreement include, but are not limited to: are some streams items ordered, or are all stream items out of order? Are streams parallelized explicitly (via syntax) or implicitly (by the system)? Can parallel instances of a stream operator communicate via external state or communicate with external services? Can parallel instances communicate with each other, through mechanisms such as broadcasting a message to all other instances? We will discuss some of these differences in more detail in \Cref{cha:background}. In summary, \emph{there is no common, agreed upon semantics.} The lack of a common language and semantics makes it difficult to design formal tool support, especially if we want the ideas to be applicable across systems.

Besides semantic bugs, misunderstanding the details of parallelism in the system is also an easy way to make performance bugs. Performance is critical and usually excellent, but is not formally guaranteed; it can be far less predictable and reliable than performance at the level of hardware or even operating systems.

Semantic language-level issues do not constitute the only barriers to software development in today's stream processing systems. Many other user-facing problems are of critical importance -- to name a few, debugging tool support, boilerplate code and configuration, input and output, and interfacing with the system and with other services. We do not focus on these problems in this thesis. From personal experience programming in stream processing frameworks, we believe that semantic language-level issues do pose a significant barrier for many tasks.

In short, considered as systems, today's stream processing platforms are really quite effective -- thanks to decades of engineering and research advances. They scale automatically across threads and distributed devices, they are high-throughput and meet microsecond-level latency requirements, and they seemlessly execute simple data processing tasks (e.g. windowing, mapping, grouping, and aggregating). Considered as programming frameworks, however, they fall short of today's standards. If adopted, tools for formal correctness properties (including type safety and determinism) could revolutionize software development in stream processing frameworks.

\section{A Programming Example}

To further motivate our approach, consider the following concrete, very simple but useful example.
Two streams arrive in the system: a parallel stream of \emph{values}, which are simple integers (\texttt{int}), and a stream of \emph{barriers}, which are of the unit type (\texttt{unit}).
All of these events are timestamped when they arrive.
The values are parallel in the sense that they may arrive in the system in parallel at multiple nodes.
The barriers all arrive in one stream at a single node.
Our task is to output the ``sum of the values occuring bewteen every two adjacent barriers.'' That is, whenever a barrier occurs, we want to output the sum of all the values with timestamp values since the previous barrier. This computation is sometimes known as an \emph{event-based window} because the window of events to aggregate depends on the occurence or certain events (in this case, the occurence of a barrier).
% TODO Cite
% https://www.vertica.com/docs/9.3.x/HTML/Content/Authoring/AnalyzingData/SQLAnalytics/Event-basedWindows.htm#:~:text=Event%2Dbased%20windows%20let%20you,as%20triggers%20to%20other%20activity.

It turns out that such a parallel computation is rather subtle and challenging to program in existing systems. High-level query libraries (e.g., based on SQL) typically don't provide event-based windows directly but would require deriving them from expensive non-parallelizable joins.
\Naive{}ly, one way to solve the problem is to send all the values to the same node as the barrier, but this results in a central bottleneck and does not exploit any parallelism.
To solve the problem, one solution is to \emph{broadcast} stream to all other parallel nodes; the broadcast primitive is provided in a few systems such as Flink and Timely Dataflow.
% TODO Cite
Once the stream is visible from all parallel nodes, each of those streams then has to solve only a local windowing problem.
The local event-based window can be achieved in multiple ways, depending on the system; at the core, it requires a re-timestamping operation to
group value events with respect to the barrier-window that they fall into,
where the window is either identified by a number in sequence (1 for the first window, 2 for the second, and so on) or by the timestamp of the barrier it corresponds to.
After values are timestamped appropriately, they can be added by-timestamp (a built-in operation in all systems) and then aggregated across parallel nodes (also standard) for the final output at each barrier.

The value-barrier example is simple, but emblematic of the broader problem: when parallel structure is not simply embarrassingly-parallel and requires some synchronization (in this case, synchronization based on the barrier stream), embarrassingly-parallel abstractions fail.
We claim that there is a better way; that parallelism can be expressed in a more semantically meaningful manner at a higher-level of abstraction.
Going back to the problems we want to address in existing systems:
\begin{itemize}
  \item \emph{Type-safety:} Fundamentally, the value stream and the barrier stream are quite different. The barrier stream arrives only at one node; the value stream arrives in parallel at many nodes. To our knowledge, no existing system distinguishes between these two kinds of streams at the typing level. So operations that interact between sequential streams and parallel streams are fundamentally not type-safe: a consumer expecting a sequential stream may get a parallel one at many nodes in case of a software bug.
  \item \emph{Determinism:} In the ``correct'' computation, the output value at each barrier is a deterministic function of the inputs (including their timestamps). In our experience it is very easy to write this computation incorrectly and accidentally window values in a nondeterministic manner, e.g. if events are grouped as-they-arrive instead of by timestamp. Yet the only way to detect this sort of nondeterminism is to run the system and hope that an execution appears where the events arrive in a different order than the timestamps indicate, which is highly unlikely when the rate of events is sparse and only becomes likely under heavy input load. In practice this becomes a highly difficult-to-detect error in production.
  \item \emph{Performance:} Finally, it is very easy to write a version of this computation (such as the \naive{} version that sends everything to one node) that is inefficient. In an ideal world, systems would give some formal guarantees about performance through static information known at compile-time, and flag an error if parallelism is being ignored entirely or if there is a sequential bottleneck.
\end{itemize}

Much of the work in this thesis stems from a desire to go beyond primitive parallelism: where primitive parallelism includes ``everything is unordered'', ``everything is ordered'', or ``events are partiioned by key and ordered for each specific key.'' Incorporating only these three kinds of parallelism represents the state-of-the-art, but is ad hoc and does not fare well in examples such as the value-barrier where there is inter-dependent ordering between events.

\section{Our Approach}

Towards more reliable abstractions for processing streams, we begin with a foundational question: what is a \emph{stream}?
In this thesis, we investigate a view of \emph{streams as partially ordered sets}.
In this view, events in the system constitute a partially ordered set,
where two events are viewed as unordered if their order
is not guaranteed by the system.
We approach the problem of \emph{specifying} these partial orders type-theoretically: we begin in \Cref{cha:foundation} by
outlining a type system for streams which serves as a foundation for the rest of the thesis.
This partial order viewpoint is closely related to the notion of \emph{dependence relations} studied in concurrency theory.
% TODO cite
All the other sections of the thesis can be seen as investigating transformations on streams as given by the types in \Cref{cha:foundation} from different semantic view points.


Historically, we defined two typing disciplines for partially ordered streams:
data-trace types~\citeMain{festschrift18,pldi19},
and synchronization schemas~\citeMain{pods21}.
This thesis presents a small refinement of the latter view (slightly removing some baggage), distilling the core type system involved.
Once the core type system for streams is defined,
in \Cref{cha:composition}, we consider how to define operators over streams compositionally~\citeMain{pods21}.
In \Cref{cha:distribution}, we go further by considering how to automatically distribute operators in a way that is \emph{safe} -- i.e., a way that guarantees determinism~\citeMain{ppopp22}.
In \Cref{cha:monitoring}, we consider operators defined over sequences of events, rather than compositionally based on the structure of the stream; we consider a finite-state transducer model for quantitatively processing such streams. This specifically solves the problem of providing \emph{performance guarantees} for stream operators~\citeMain{popl19,icalp17,tcs20}.

Specifically, we aim to offer provable guarantees about performance: in particular considering finite-state models of stream processing systems~\citeMain{icalp17,popl19,tcs20}.
In particular, we target \emph{runtime monitoring} applications
over data streams, and we show how to define and compile runtime monitors
with provable performance.
These can be used either to monitor DSPS applications on the side,
or to define DSPS operators themselves.
While the existing work has been used for compilation of high-level query languages on a single machine with space and time bounds~\citeMain{popl19}~\cite{QRE,StreamQRE},
the primary direction for future work here is to adapt to the \emph{distributed setting} and show similar performance guarantees there.

In addition to static typing guarantees
we use the partial order viewpoint for differential testing~\citeMain{oopsla20}.
The motivation for this is that while static types are useful
for new applications, existing applications are not developed with the new
framework and so can benefit from runtime testing to identify bugs due to
output even reordering.

We will discuss related work in more detail in \Cref{cha:rw}, but a few lines of work have been particularly influential enough on our work to call them out in the introduction.
The theory of Mazurkiewicz traces~\cite{mazurkiewicz1986trace,DiekertR1995} is central to our study of partially ordered streams.
On the programming language level, we view Kahn Process Networks~\cite{gilles1974semantics} as the foundational model for deterministic concurrent dataflow programming; StreamIt~\cite{thies2002streamit} constitutes the most successful previous effort to date at a principled language design, based on the restriction of Kahn's networks to Synchronous Dataflow~\cite{lee1987synchronous}.
The work at IBM on safe parallelism and the SPL language~\cite{HAG2013SPL,schneider2013safe,hirzel2014catalog} also deserves mention.
On the formal languages side,
the theory of finite-state transducers in general~\cite{EH2001MDST,AC2010SST}
and quantitative automata in particular~\cite{S1961WA,DKV2009HWA,AdADRY2013CRA}
have played a direct role in the development of our performance-bounded
model in \Cref{cha:monitoring}.
Finally, the Continuous Query Language~\cite{CQL,ABW2006CQL} has been an ongoing source of inspiration.

\section{Contributions}

This thesis contributes the following:

\begin{itemize}
\item
\emph{Semantics:}
We propose \emph{partial order semantics} for DSPS,
through the data-trace types~\citeMain{pldi19} and synchronization schemas~\citeMain{pods21} typing disciplines.
We show how viewing streams as partially ordered allows for deterministic semantics,
and show how well-typed programming disciplines can be developed,
leading to well-defined DSPS programming libraries.
(\Cref{cha:composition})

\item
\emph{Testing:}
To detect bugs due to nondeterminism in existing DSPS applications,
we propose DiffStream, a differential testing framework~\citeMain{oopsla20}.
In particular, we leverage the partial order viewpoint to specify
ordering requirements, and we test for violations at runtime.
(\Cref{cha:testing})

\item
\emph{Safe distribution:}
For more complex application logic where currently error-prone
low-level state management is used,
we propose \emph{dependency-guided synchronization},
a programming framework for \emph{semantics-preserving} distribution~\citeMain{ppopp22}.
A key aspect of this work is the ability to program
synchronization between distributed nodes, which is not supported
by most existing systems,
while still ensuring safety and allowing automatic distribution
of streaming applications.
(\Cref{cha:distribution})

\item
\emph{Monitoring:}
Towards streaming applications with predictable performance,
we propose data transducers, a monitoring formalism and state-machine based intermediate representation~\citeMain{popl19}.
Our formalism is compositional, enabling compilation of high-level
monitoring queries with provable performance bounds.
(\Cref{cha:monitoring})

\item
\emph{Verification:}
As future work, we propose a verified streaming library
over partially ordered streams, incorporating ideas from synchronization schemas~\citeMain{pods21} and data transducers~\citeMain{popl19}.
The library enables a high-level query language which is well-typed,
where typing properties are checked partly statically and partly by
generating external verification conditions.
(Section~\ref{cha:discussion})
\end{itemize}

\section{Software}
