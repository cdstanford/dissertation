\section{Introduction}
\label{sec:intro}

Today, data is produced at an overwhelming rate
that cannot be processed by traditional methods.
For example, Cisco has estimated in its annual white paper
that data produced by people, machines, and things
is around 500 zettabytes, in contrast to a much smaller volume
of data that can be feasibly stored~\cite{index2018forecast}.
Reachers and industry practice have accordingly recognized the demand
for a new paradigm of computing where data is
distributed (processed in parallel over many devices),
transient (processed as it arrives and discarded),
and temporally structured (considered with respect to time).
This \emph{stream processing} paradigm has given rise to an increasing number
of modern data processing software platforms.\footnote{E.g.: Kinesis from Amazon, Timely and Differential Dataflow from Materialize, and a range of open-source frameworks including Spark, Storm, Flink, Kafka, Samza, Heron, and Calcite from the Apache Software Foundation.}
Broadly construed, the stream processing paradigm is also recognized by designers of other modern systems: these include microservices deployed in the cloud; IoT and other edge devices, which operate in response
to sensor data~\cite{shi2016edge, ashton2009internet}; and even programmable network switches,
which aim to push some of this expensive streaming computation
into the network.
Stream processing also has theoretical justification in the \emph{streaming model of computation},
where items arrive one at a time and are processed as they arrive, using
an ideally minimal amount of space and time per element.
In this thesis, we investigate the stream processing paradigm from the lens of \emph{programming languages}, by introducing software abstractions which are type-safe, deterministic, and performant: that is, which save programmers from common mistakes and ensure that the program that is deployed meets their intent.

\subsection{Motivation}

Despite the widespread interest in stream processing, researchers have devoted inadequate attention to software correctness and safety~\citeMain{stanford2022correctness}.
Consdiered as systems, today's stream processing platforms are really quite effective: they scale automatically across threads and devices, they are high-throughput, and for most simple data processing tasks (e.g. windowing, mapping, grouping, and aggregating) the details of how streams are parallelized and distributed are completely hidden. Considered as programming frameworks, however, they fall short of today's standards -- as we will discuss further in \Cref{sec:background}. The reason for this gap is largely due to the problem of parallelism and concurrency. Different stream processing frameworks make wildly different decisions as to basic details, including whether streams are parallelized explicitly or implicitly; whether items are fully out-of-order or partially out-of-order; whether items are guaranteed to be processed as they arrive or may be delayed into batches; and whether synchronization is possible between parallel nodes or whether they must compute independently.
In brief, \emph{there is no common, agreed upon semantics for parallel streams.}
This results in at least two undesirable features: first, the lack of guaranteed \emph{determinism} in all existing systems when viewing or processing a stream as its individual elements; and second, the lack of \emph{type safety} with respect to the type of parallelism present in streams, as well as the constraints on expected input items.

\subsection{Our approach}

Towards more reliable abstractions for processing streams, we begin with a foundational question: what is a \emph{stream}?
In this thesis, we investigate a view of \emph{streams as partially ordered sets}.
In this view, events in the system constitute a partially ordered set,
where two events are viewed as unordered if their order
is not guaranteed by the system.
We approach the problem of \emph{specifying} these partial orders type-theoretically: we begin in \Cref{sec:foundation} by
outlining a type system for streams which serves as a foundation for the rest of the thesis.
This partial order viewpoint is closely related to the notion of \emph{dependence relations} studied in concurrency theory,
All the other sections of the thesis can be seen as investigating transformations on streams as given by the types in \Cref{sec:foundation} from different semantic view points.

Much of our work on partially ordered streams stems from a desire to accomodate parallelism in a less ad hoc way than the standard options of just ``everything is fully parallel'', ``everything is ordered'', or ``events are partiioned by key and ordered for each specific key''.
Consider the following simple illustrative example.
% TODO: discuss the value-barrier example here.

Historically, we defined two typing disciplines for partially ordered streams:
data-trace types~\citeMain{festschrift18,pldi19},
and synchronization schemas~\citeMain{pods21}.
This thesis presents a small refinement of the latter view (slightly removing some baggage), distilling the core type system involved.
Once the core type system for streams is defined,
in \Cref{sec:composition}, we consider how to define operators over streams compositionally~\citeMain{pods21}.
In \Cref{sec:distribution}, we go further by considering how to automatically distribute operators in a way that is \emph{safe} -- i.e., a way that guarantees determinism~\citeMain{ppopp22}.
In \Cref{sec:monitoring}, we consider operators defined over sequences of events, rather than compositionally based on the structure of the stream; we consider a finite-state transducer model for quantitatively processing such streams. This specifically solves the problem of providing \emph{performance guarantees} for stream operators~\citeMain{popl19,icalp17,tcs20}.

Specifically, we aim to offer provable guarantees about performance: in particular considering finite-state models of stream processing systems~\citeMain{icalp17,popl19,tcs20}.
In particular, we target \emph{runtime monitoring} applications
over data streams, and we show how to define and compile runtime monitors
with provable performance.
These can be used either to monitor DSPS applications on the side,
or to define DSPS operators themselves.
While the existing work has been used for compilation of high-level query languages on a single machine with space and time bounds~\citeMain{popl19}~\cite{QRE,StreamQRE},
the primary direction for future work here is to adapt to the \emph{distributed setting} and show similar performance guarantees there.

In addition to static typing guarantees
we use the partial order viewpoint for differential testing~\citeMain{oopsla20}.
The motivation for this is that while static types are useful
for new applications, existing applications are not developed with the new
framework and so can benefit from runtime testing to identify bugs due to
output even reordering.

We will discuss related work in more detail in \Cref{sec:rw}, but a few lines of work have been particularly influential enough on our work to call them out in the introduction.
The theory of Mazurkiewicz traces~\cite{mazurkiewicz1986trace,DiekertR1995} is central to our study of partially ordered streams.
On the programming language level, we view Kahn Process Networks~\cite{gilles1974semantics} as the foundational model for deterministic concurrent dataflow programming; StreamIt~\cite{thies2002streamit} constitutes the most successful previous effort to date at a principled language design, based on the restriction of Kahn's networks to Synchronous Dataflow~\cite{lee1987synchronous}.
The work at IBM on safe parallelism and the SPL language~\cite{HAG2013SPL,schneider2013safe,hirzel2014catalog} also deserves mention.
On the formal languages side,
the theory of finite-state transducers in general~\cite{EH2001MDST,AC2010SST}
and quantitative automata in particular~\cite{S1961WA,DKV2009HWA,AdADRY2013CRA}
have played a direct role in the development of our performance-bounded
model in \Cref{sec:monitoring}.
Finally, the Continuous Query Language~\cite{CQL,ABW2006CQL} has been an ongoing source of inspiration.

\subsection{Contributions}

This thesis contributes the following:

\begin{itemize}
\item
\emph{Semantics:}
We propose \emph{partial order semantics} for DSPS,
through the data-trace types~\citeMain{pldi19} and synchronization schemas~\citeMain{pods21} typing disciplines.
We show how viewing streams as partially ordered allows for deterministic semantics,
and show how well-typed programming disciplines can be developed,
leading to well-defined DSPS programming libraries.
(\Cref{sec:composition})

\item
\emph{Testing:}
To detect bugs due to nondeterminism in existing DSPS applications,
we propose DiffStream, a differential testing framework~\citeMain{oopsla20}.
In particular, we leverage the partial order viewpoint to specify
ordering requirements, and we test for violations at runtime.
(\Cref{sec:testing})

\item
\emph{Safe distribution:}
For more complex application logic where currently error-prone
low-level state management is used,
we propose \emph{dependency-guided synchronization},
a programming framework for \emph{semantics-preserving} distribution~\citeMain{ppopp22}.
A key aspect of this work is the ability to program
synchronization between distributed nodes, which is not supported
by most existing systems,
while still ensuring safety and allowing automatic distribution
of streaming applications.
(\Cref{sec:distribution})

\item
\emph{Monitoring:}
Towards streaming applications with predictable performance,
we propose data transducers, a monitoring formalism and state-machine based intermediate representation~\citeMain{popl19}.
Our formalism is compositional, enabling compilation of high-level
monitoring queries with provable performance bounds.
(\Cref{sec:monitoring})

\item
\emph{Verification:}
As future work, we propose a verified streaming library
over partially ordered streams, incorporating ideas from synchronization schemas~\citeMain{pods21} and data transducers~\citeMain{popl19}.
The library enables a high-level query language which is well-typed,
where typing properties are checked partly statically and partly by
generating external verification conditions.
(Section~\ref{sec:discussion})
\end{itemize}

\subsection{Software}
